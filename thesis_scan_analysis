import numpy as np
from numpy.core.shape_base import block
import pandas as pd
from ibl_pipeline.utils import psychofit
import query_scan_mice as query
import seaborn as sns
import matplotlib.pyplot as plt
from math import ceil
from scipy import stats
import matplotlib as mpl
import functools
import numpy as np
import scipy.optimize
from scipy.special import erf, erfc
import platform

def mle_fit_psycho(data, P_model='weibull', parstart=None, parmin=None, parmax=None, nfits=50):
    """
    Maximumum likelihood fit of psychometric function.

    Args:
        data: 3 x n matrix where first row corrsponds to stim levels (%), 
            the second to number of trials for each stim level (int),
            the third to proportion correct (float between 0 and 1)
        P_model: The psychometric function. Possibilities include 'weibull'
            (DEFAULT), 'weibull50', 'erf_psycho' and 'erf_psycho_2gammas'
        parstart: Non-zero starting parameters, used to try to avoid local
            minima.  The parameters are [threshold, slope, gamma], or if
            using the 'erf_psycho_2gammas' model append a second gamma value.
            Recommended to use a value > 1.
            If None, some reasonable defaults are used.
        parmin: Minimum parameter values.  If None, some reasonable defaults 
            are used
        parmax: Maximum parameter values.  If None, some reasonable defaults 
            are used
        nfits: the number of fits

    Returns:
        pars: The parameters from the best of the fits
        L: The likliehood of the best fit
        
    Raises:
        TypeError: data must be a list or numpy array
        ValueError: data must be m by 3 matrix

    Examples:
        Below we fit a Weibull function to some data:
        
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt
        >>> cc = np.array([-8., -6., -4., -2.,  0.,  2.,  4.,  6.,  8.]) # contrasts
        >>> nn = np.full((9,),10) # number of trials at each contrast
        >>> pp = np.array([5., 8., 20., 41., 54., 59., 79., 92., 96])/100 # proportion "rightward"
        >>> pars, L = mle_fit_psycho(np.vstack((cc,nn,pp)), 'erf_psycho')
        >>> plt.plot(cc, pp, 'bo', mfc='b')
        >>> plt.plot(np.arange(-8,8,0.1), erf_psycho(pars,np.arange(-8,8,0.1)), '-b')
        
    Information:
        1999-11 FH wrote it
        2000-01 MC cleaned it up
        2000-04 MC took care of the 50% case
        2009-12 MC replaced fmins with fminsearch
        2010-02 MC, AZ added nfits
        2013-02 MC+MD fixed bug with dealing with NaNs
        2018-08 MW ported to Python
        2019-12 CK added cumulative normal function
    """
    # Input validation
    if isinstance(data, (list, tuple)):
        data = np.array(data)
    elif not isinstance(data, np.ndarray):
        raise TypeError('data must be a list or numpy array')

    if data.shape[0] != 3:
        raise ValueError('data must be m by 3 matrix')

    if parstart is None:
        parstart = np.array([np.mean(data[0,:]), 3., .05])
    if parmin is None:
        parmin = np.array([np.min(data[0,:]), 0., 0.])
    if parmax is None:
        parmax = np.array([np.max(data[0,:]), 10., .4])

    # find the good values in pp (conditions that were effectively run)
    ii = np.isfinite(data[2,:])

    likelihoods = np.zeros(nfits,)
    pars = np.empty((nfits,parstart.size))
    
    f = functools.partial(neg_likelihood, data=data[:,ii], 
                          P_model=P_model, parmin=parmin, parmax=parmax)
    for ifit in range(nfits):
        pars[ifit,:] = scipy.optimize.fmin(f, parstart, disp=False)
        parstart = parmin + np.random.rand(parmin.size) * (parmax-parmin)
        likelihoods[ifit] = - neg_likelihood(pars[ifit,:], data[:,ii], P_model, parmin, parmax)

    # the values to be output
    L = likelihoods.max()
    iBestFit = likelihoods.argmax()
    return pars[iBestFit,:], L

def neg_likelihood(pars, data, P_model='psych_cumNorm', parmin=None, parmax=None):
    """
    Negative likelihood of a psychometric function.

    Args:
        pars: Model parameters [threshold, slope, gamma], or if
            using the 'erf_psycho_2gammas' model append a second gamma value.
        data: 3 x n matrix where first row corrsponds to stim levels (%), 
            the second to number of trials for each stim level (int),
            the third to proportion correct (float between 0 and 1)
        P_model: The psychometric function. Possibilities include 'weibull'
            (DEFAULT), 'weibull50', 'erf_psycho' and 'erf_psycho_2gammas'
        parmin: Minimum bound for parameters.  If None, some reasonable defaults 
            are used
        parmax: Maximum bound for parameters.  If None, some reasonable defaults 
            are used

    Returns:
        l: The likliehood of the parameters.  The equation is:
            - sum(nn.*(pp.*log10(P_model)+(1-pp).*log10(1-P_model)))
            See the the appendix of Watson, A.B. (1979). Probability
            summation over time. Vision Res 19, 515-522.
        
    Raises: 
        ValueError: invalid model, options are "weibull", 
                    "weibull50", "erf_psycho" and "erf_psycho_2gammas"
        TypeError: data must be a list or numpy array
        ValueError data must be m by 3 matrix
        
    Information:
        1999-11 FH wrote it
        2000-01 MC cleaned it up
        2000-07 MC made it indep of Weibull and added parmin and parmax
        2018-08 MW ported to Python
    """
    # Validate input
    if isinstance(data, (list, tuple)):
        data = np.array(data)
    elif not isinstance(data, np.ndarray):
        raise TypeError('data must be a list or numpy array')
        
    if parmin is None:
        parmin = np.array([.005, 0., 0.])
    if parmax is None:
        parmax = np.array([.5, 10., .25])
        
    if data.shape[0] == 3:
        xx = data[0,:]
        nn = data[1,:]
        pp = data[2,:]
    else:
        raise ValueError('data must be m by 3 matrix')

    # here is where you effectively put the constraints.
    if (any(pars < parmin)) or (any( pars > parmax)):
        l = 10000000
        return l

    dispatcher={'psych_cumNorm': psych_cumNorm, 'erf_psycho_2gammas' : erf_psycho_2gammas}
    try:
        probs = dispatcher[P_model](pars,xx)
    except KeyError:
        raise ValueError('invalid model, options are "weibull", '+
                         '"weibull50", "erf_psycho" and "erf_psycho_2gammas"')

    assert (max(probs)<=1) or (min(probs) >= 0),'At least one of the probabilities is not between 0 and 1'

    probs[probs==0]=np.finfo(float).eps
    probs[probs==1]=1-np.finfo(float).eps

    l = - sum(nn*(pp*np.log(probs)+(1-pp)*np.log(1-probs)))
    return l
def erf_psycho_2gammas(pars, xx):
    """
    erf function from 0 to 1, with two lapse rates.

    Args:
        pars: Model parameters [threshold, slope, gamma].
        xx: vector of stim levels (%)

    Returns:
        ff: A vector of length xx
        
    Examples:
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt
        >>> xx = np.arange(-50,50)
        >>> ff = erf_psycho_2gammas(np.array(-10., 10., 0.2, 0.),xx)
        >>> plt.plot(xx,ff)
        
    Raises: 
        ValueError: pars must be a vector of length 3
        ValueError: each of the three parameters must be scalar
        TypeError: pars must be a list or numpy array
        
    Information:
        2000    MC wrote it
        2018-08 MW ported to Python
    """
    # Validate input
    if isinstance(pars, (list, tuple)):
        pars = np.array(pars)
    elif not isinstance(pars, np.ndarray):
        raise TypeError('pars must be a list or numpy array')

    if pars.shape[0] != 4:
        raise ValueError('pars must be a vector of length 4')
    threshold	= pars[0]
    slope	= pars[1]
    gamma1	= pars[2]
    gamma2	= pars[3]

    if (threshold.size!=1) or (slope.size!=1) or (gamma1.size!=1) or (gamma2.size!=1):
        ValueError('each of the three parameters must be scalar')
    
    return gamma1 + (1 - gamma1 - gamma2) * (erf( (xx-threshold)/slope ) + 1 )/2
def psych_cumNorm(params, x):
    '''
    Psychometric function based on the cumulative normal distribution. inputs and returns a param
    vector [mu, invSigma, gamma, lambda] or [category boundary, inverse slope, low lapse, high lapse]
    '''
    mu = params[0]  # mean of the gaussian, x value for chance level
    invSigma = params[1]  # inverse variance of gaussian, aka inverse slope of psycho
    gamma = params[2]  # lapse Low
    lamb = params[3]  # lapse High
    y = gamma + (1 - gamma - lamb) * .5 * erfc(-invSigma *(x-mu) / np.sqrt(2))
    return y

def plot_from_spots(spotlist, psychoSpotData, spots, color, ax, plotType='psycho', bs=False):
    """
        Function to plot three psychometrics on a single plot, with the dots that they were 
        generated with. usually I'll use this with a group of left spots, a group of right spots
        and a group of control spots.

        Inputs:
        spotlists: an array containing the spots that you want to group together to plot the 
        psychometric for. a n by 2 numpy array that gives [[x1,y1],[x2,y2],[xn,yn]]
        psychoSpotData: a list of len(numSpots) each with a list lenght 4 containing 1, an array of
        signed contrasts, 2, a list of number of presentations for that contrast, 3, a list of 
        arrays containing a bool for if choice was CCW (list len num contrast, array len num
        presentations) 4, a list of the same form as above with the reaction times
        spots: a dataframe of length num spots, column 0: laserPosX, column 1: laserPosY, column3, 
        count
        color: the color for the line you want to plot, string eg 'b'
        ax: the matplotlib axis to plot onto
        plotType: defualt is psycho for plotting psychometric curves, other option is 'chrono'

    """
    if plotType == 'psycho':
        psycho = [[],[0 for i in range(len(psychoSpotData[0][1]))],[np.array([])]*len(psychoSpotData[0][1])]
        controlSpotPsycho = [[],[0 for i in range(len(psychoSpotData[0][1]))],[np.nan for i in range(len(psychoSpotData[0][1]))]]
        tempPsych = [np.array([])]*len(psychoSpotData[0][1])
        for i in range(len(spots)):
            spotX = spots.iloc[i, [0]][0]
            spotY = spots.iloc[i, [1]][0]
            for spot in range(len(spotlist)):
                if spotX == spotlist[spot][0] and spotY == spotlist[spot][1]:
                    psycho[0] = psychoSpotData[i][0]
                    psycho[1] = [temp+j for temp,j in zip(psychoSpotData[i][1],psycho[1])]
                    for contrast in range(len(psycho[0])):
                        con = int(contrast)
                        tempPsych[con] = np.append(tempPsych[con], psychoSpotData[i][2][con])

        sems = []
        for c in range(len(tempPsych)):
            psycho[2][c] = np.nanmean(tempPsych[c])
            sems.append(stats.sem(tempPsych[c]))

        if bs:
        ## Bootstrap confidence intervals
            nboots = 500
            bootFits = pd.DataFrame(columns=['threshold','slope','gamma','lambda'], index=range(nboots))
            bootData = [[],[0 for i in range(len(psychoSpotData[0][1]))],[np.array([])]*len(psychoSpotData[0][1])]
            bootData[0] = psycho[0]
            cnt=0
            print('bootstrapping errorbars...', sep=' ', end='')
            for i in range(nboots):
                if not(cnt % 5):
                    print(int(cnt/nboots*100), sep=' ', end='%,', flush=True)
                for j in range(len(tempPsych)):
                    bootData[2][j] = np.random.choice(tempPsych[j],size=int(len(tempPsych[j])/1.25), replace=True)                
                    bootData[1][j] = len(bootData[2][j])
                    bootData[2][j] = np.mean(bootData[2][j])
                fitParams = []
                fitLikes = []
                for repeat in range(10):
                    parStart = np.array([-5+np.random.rand()*10,0+np.random.rand()*100, 0+np.random.rand(), 0+np.random.rand()])
                    pars, L = mle_fit_psycho(bootData, P_model='erf_psycho_2gammas',
                                                                parstart=np.array([0, 50, .5, .5]),
                                                                parmin=np.array([-5, 0., 0., 0.]),
                                                                parmax=np.array([5, 100., 1, 1]),
                                                                nfits=20)
                    

                a = .05
                CIs = []
                for i in bootFits.columns:
                    CIs.append([np.percentile(bootFits[i],100-a/2), np.percentile(bootFits[i],a/2,)])
        else:
            CIs = None        


            ## plotting psychometrics for different cortical groups
        lines = []
        
        fitParams = []
        fitLikes = []
        for repeat in range(10):
            parStart = np.array([-5+np.random.rand()*10,0+np.random.rand()*100, 0+np.random.rand(), 0+np.random.rand()])
            params, L = mle_fit_psycho(psycho,
                                        P_model='erf_psycho_2gammas',
                                        parstart=parStart,
                                        parmin=np.array([-5, 0., 0., 0.]),
                                        parmax=np.array([5, 100., 1, 1]),
                                        nfits=25)
            fitParams.append(params)
            fitLikes.append(L)
            # find the best params (with the lowest neg likelihood)
        params = fitParams[np.where(min(fitLikes))[0][0]]

        spotFits.append(params)
        #plot the psychometrics
        fitx = np.linspace(-1, 1, 100)
        fity = erf_psycho_2gammas(params, fitx)

        
        line = ax.plot(fitx, fity, color=color)
        lines.append(line)
        ax.errorbar(psycho[0], np.array(psycho[2]),yerr=sems, color=color, marker='.',ms=4, ls='')
        plt.ylim(-0.1,1.1)
        plt.xlim(-1.1,1.1)
    
    elif plotType == 'chrono':
        chrono = [[],[0 for i in range(len(psychoSpotData[0][1]))],[np.array([])]*len(psychoSpotData[0][1])]
        tempChrono = [np.array([])]*len(psychoSpotData[0][1])
        for i in range(len(spots)):
            spotX = spots.iloc[i, [0]][0]
            spotY = spots.iloc[i, [1]][0]
            
            for spot in range(len(spotlist)):
                if spotX == spotlist[spot][0] and spotY == spotlist[spot][1]:
                    chrono[0] = psychoSpotData[i][0]
                    chrono[1] = [temp+j for temp,j in zip(psychoSpotData[i][1],chrono[1])]
                    for contrast in range(len(chrono[0])):
                        con = int(contrast)
                        tempChrono[con] = np.append(tempChrono[con], psychoSpotData[i][3][con])
        sems = []
        for c in range(len(tempChrono)):
            chrono[2][c] = np.nanmedian(tempChrono[c])
            sems.append(stats.sem(tempChrono[c]))
        ax.errorbar(chrono[0], np.array(chrono[2]), yerr=sems, color=color, marker='.',ms=4)
        ax.set_ylim(.15,.5)
# the params I use here are RT at 0 contrast, 'RT bias' which is pairwise RT left- RT right, and 
# peakiness which is the ratio of max RT to the average of the two 100% RTs
        p2 = (chrono[2][0] - chrono[2][-1]) + (chrono[2][1] - chrono[2][-2] + (chrono[2][2] - chrono[2][-3]) + (chrono[2][3] - chrono[2][-4]))
        params = [chrono[2][4], p2, max(chrono[2])/np.mean([chrono[2][0],chrono[2][-1]]), chrono[2]] 
        CIs = None
    else:
        raise Exception("This is not a supported plot type, choose 'psycho' or 'chrono'")
    return params, CIs

def err_bar_from_CIs(allCIs, x, y, ax):
    if all(allCIs.iloc[0]):
        for i in range(len(allCIs)):
            ax.axvline(x=allCIs[x][i], ymin=allCIs[y][i][1], ymax=allCIs[y][i][0],lw=1,color='k')
        
###########################################################################################
################################# Start of script #########################################
###########################################################################################

bregma = [228.5, 190]
pixelsize = .025  # mm
if platform.system() == 'Darwin':
    allenOutline = np.load('/Users/ckrasnia/Desktop/Zador_Lab/scanData/allen_dorsal_outline')
else:
    allenOutline = np.load(r'F:\allen_dorsal_outline')
data = query.align_laser2behavior(['CSK-scan-014','CSK-scan-015','CSK-scan-016', 'CSK-scan-019'])
# ['CSK-scan-023','CSK-scan-024','CSK-scan-025','CSK-scan-026']
sub_names = ['CSK-scan-014','CSK-scan-015','CSK-scan-016', 'CSK-scan-019','All subjects']
bigData = [pd.concat(data)]
data.append(bigData[0])
for sub in range(len(data)):
    bigData=data[sub]
    drop=(bigData['trial_response_choice']!= 'No Go')# & (np.abs(bigData['signed_contrast']) >= .25)
    bigData=bigData[drop]
    bigData['CCW']=bigData['trial_response_choice']=='CW'
    spots = bigData.groupby(['laserPosX', 'laserPosY']).size().reset_index().rename(columns={0: 'count'})
    spots_RTs = np.zeros(len(spots))
    spots_RTp = np.zeros(len(spots))
    control_RTs = np.array(bigData[bigData['laserPosY']<-6]['trial_response_time']-bigData[bigData['laserPosY']<-6]['trial_go_cue_trigger_time'])
    CCWp = np.zeros(len(spots))
    spots_CCW = np.zeros(len(spots))
    control_CCW = np.array(bigData[bigData['laserPosY']<-6]['CCW'])
    for i,spot in spots.iterrows():
        laserX=spot['laserPosX']
        laserY=spot['laserPosY']
        n=spot['count']
        spot_data=bigData[(bigData['laserPosX']==laserX) & (bigData['laserPosY']==laserY)]
        spots_CCW[i] = np.nanmean(np.array(spot_data['CCW'].astype(bool)))
        CCWp[i] = stats.ranksums(control_CCW,spot_data['CCW'])[-1]
        # CCWp[i] = stats.ttest_1samp(spot_data['CCW'],.5)[-1]
        RT=spot_data.trial_response_time-spot_data.trial_go_cue_trigger_time
        spots_RTs[i] = np.median(RT)
        spots_RTp[i] = stats.ranksums(control_RTs,RT)[-1]
    spots['RT']=spots_RTs
    spots['RTp']=spots_RTp
    spots['CCWp']=CCWp
    spots['CCW']=spots_CCW

    spots['size']=0
    for i,spot in spots.iterrows():
        if spot['RTp'] <= .0001 / len(spots):
            spot_size = 300
        elif spot['RTp'] <= .001 / len(spots):
            spot_size = 175
        elif spot['RTp'] <= .01 / len(spots):
            spot_size = 100
        else:
            spot_size = 5
        spots['size'].iloc[i] = spot_size

    spots['allenX'] = (spots['laserPosX'] / pixelsize) + bregma[0]
    spots['allenY'] = (spots['laserPosY'] / -pixelsize) + bregma[1]

    fig,axs = plt.subplots(2,1,gridspec_kw={'height_ratios': [9, 1]})
    norm = mpl.colors.Normalize(vmin=-20, vmax=20)

    axs[0].imshow(allenOutline, cmap="gray", interpolation='nearest')

    axs[0] = sns.scatterplot(ax=axs[0],data=spots,x='allenX', y='allenY', size='size', sizes=(20, 400),
                            hue=(((np.array(spots['RT'])-(np.median(control_RTs)))/np.median(control_RTs))*100),
                            palette='RdBu_r', legend=False, edgecolor='k',hue_norm=norm)
    text_spots = spots[spots['size']==300]
    text=np.round((((np.array(text_spots['RT'])-(np.median(control_RTs)))/np.median(control_RTs))*100)).astype(int)
    for i in range(len(text)):
        axs[0].text(text_spots['allenX'].iloc[i],text_spots['allenY'].iloc[i],str(text[i]),ha='center',va='center')
    axs[0].axis('off')
    if len(text)>0:
        cb1 = mpl.colorbar.ColorbarBase(axs[1], cmap=mpl.cm.get_cmap('RdBu_r'),
                                        norm=mpl.colors.Normalize(vmin=-25, vmax=25),orientation='horizontal',ticks=np.linspace(-25,25,5))
        cb1.set_label('Percent change from baseline')
    axs[0].set_title(sub_names[sub])
    plt.show(block=False)

    # for percent left
    spots['size']=0
    for i,spot in spots.iterrows():
        if spot['CCWp'] <= .0001 / len(spots):
            spot_size = 300
        elif spot['CCWp'] <= .001 / len(spots):
            spot_size = 175
        elif spot['CCWp'] <= .01 / len(spots):
            spot_size = 100
        else:
            spot_size = 5
        spots['size'].iloc[i] = spot_size

    spots['allenX'] = (spots['laserPosX'] / pixelsize) + bregma[0]
    spots['allenY'] = (spots['laserPosY'] / -pixelsize) + bregma[1]

    fig,axs = plt.subplots(2,1,gridspec_kw={'height_ratios': [9, 1]})
    norm = mpl.colors.Normalize(vmin=0, vmax=1)

    axs[0].imshow(allenOutline, cmap="gray", interpolation='nearest')

    axs[0] = sns.scatterplot(ax=axs[0],data=spots,x='allenX', y='allenY', size='size', sizes=(20, 400),
                            hue='CCW', palette='seismic', legend=False, edgecolor='k',hue_norm=norm)
    text_spots = spots[spots['size']==300]
    text=(np.array(text_spots['CCW'])*100).astype(int)
    for i in range(len(text)):
        axs[0].text(text_spots['allenX'].iloc[i],text_spots['allenY'].iloc[i],str(text[i]),ha='center',va='center',color='w')
    axs[0].text(0+bregma[0], (-7.5/ -pixelsize) + bregma[1],int(np.mean(control_CCW)*100),ha='center',va='center')
    axs[0].axis('off')
    if len(text)>0:
        cb1 = mpl.colorbar.ColorbarBase(axs[1], cmap=mpl.cm.get_cmap('seismic'),
                                        norm=mpl.colors.Normalize(vmin=0, vmax=100),orientation='horizontal',ticks=np.linspace(0,100,5))
        cb1.set_label('Percent choose left')
    axs[0].set_title(sub_names[sub])

    plt.show(block=False)


moLeftSpots = np.array([[-1.5,.5],[-1.5,1.5],[-1.5,2.5],[-2.5,1.5]])
moRightSpots = np.array([[1.5,.5],[1.5,1.5],[1.5,2.5],[2.5,1.5]])
visLeftSpots = np.array([[-2.5,-2.5],[-3.5,-2.5],[-2.5,-3.5],[-3.5,-3.5]])
visRightSpots = np.array([[2.5,-2.5],[3.5,-2.5],[2.5,-3.5],[3.5,-3.5]])
areas = [moLeftSpots,moRightSpots,visLeftSpots,visRightSpots]
area_labels = ['left motor',' right motor','left visual','right visual']
area_colors = ['r','b','r','b']

bigData=data[-1]
drop=(bigData['trial_response_choice']!= 'No Go') 
bigData=bigData[drop]
vp_fig,vp_ax = plt.subplots(1,1,figsize=(2,2))
plt.tight_layout()
mp_fig,mp_ax = plt.subplots(1,1,figsize=(2,2))
plt.tight_layout()
vc_fig,vc_ax = plt.subplots(1,1,figsize=(2,2))
plt.tight_layout()
mc_fig,mc_ax = plt.subplots(1,1,figsize=(2,2))
plt.tight_layout()
choice_RT = np.zeros((5,2))
all_choice_RT = np.zeros((5,2)).astype(object)
choice_RT_std = np.zeros((5,2))
for cnt,area in enumerate(areas):
    full_mask=np.zeros(len(bigData)).astype(bool)
    # create a mask for the dataframe of only the points we want
    for i in range(4):
        mask = (bigData['laserPosX']==area[i,0]) & (bigData['laserPosY']==area[i,1])
        full_mask = mask | full_mask
    area_data = bigData[full_mask]
    choice_RT[cnt,0] = np.median(area_data[area_data['trial_response_choice']=='CCW']['trial_response_time'] - \
                       area_data[area_data['trial_response_choice']=='CCW']['trial_go_cue_trigger_time'])
    choice_RT[cnt,1] = np.median(area_data[area_data['trial_response_choice']=='CW']['trial_response_time'] - \
                       area_data[area_data['trial_response_choice']=='CW']['trial_go_cue_trigger_time'])

    all_choice_RT[cnt,0] = area_data[area_data['trial_response_choice']=='CCW']['trial_response_time'] - \
                       area_data[area_data['trial_response_choice']=='CCW']['trial_go_cue_trigger_time']
    all_choice_RT[cnt,1] = area_data[area_data['trial_response_choice']=='CW']['trial_response_time'] - \
                       area_data[area_data['trial_response_choice']=='CW']['trial_go_cue_trigger_time']

    choice_RT_std[cnt,0] = np.std(area_data[area_data['trial_response_choice']=='CCW']['trial_response_time'] - \
                    area_data[area_data['trial_response_choice']=='CCW']['trial_go_cue_trigger_time'])/np.sqrt(len(area_data[area_data['trial_response_choice']=='CCW']))
    choice_RT_std[cnt,1] = np.std(area_data[area_data['trial_response_choice']=='CW']['trial_response_time'] - \
                       area_data[area_data['trial_response_choice']=='CW']['trial_go_cue_trigger_time'])/np.sqrt(len(area_data[area_data['trial_response_choice']=='CW']))
    # make the psychometric for this area
    stims, cnts = np.unique(area_data['signed_contrast'],return_counts=True)
    prop_CCW = []
    rts=[]
    rt_std=[]
    CCW_std = []
    for k,stim in enumerate(stims):
        stim_data = area_data[area_data['signed_contrast']==stim]
        rts.append(np.median(stim_data['trial_response_time'] - stim_data['trial_go_cue_trigger_time']))
        rt_std.append(np.std(stim_data['trial_response_time'] - stim_data['trial_go_cue_trigger_time'])/np.sqrt(cnts[k]))
        prop_CCW.append(np.mean(stim_data['trial_response_choice']=='CCW'))
        CCW_std.append(np.std(stim_data['trial_response_choice']=='CCW')/np.sqrt(cnts[k]))
    pars,lls = [],[]
    for i in range(25):
        parStart = np.array([-5+np.random.rand()*10,0+np.random.rand()*100, 0+np.random.rand(), 0+np.random.rand()])
        params, ll = mle_fit_psycho([stims,cnts,prop_CCW],P_model='erf_psycho_2gammas',
                                    parstart=parStart,
                                    parmin=np.array([-5, 0., 0., 0.]),
                                    parmax=np.array([5, 100., 1, 1]),
                                    nfits=10)
        pars.append(params)
        lls.append(ll)
    xx = np.linspace(-1,1,100)
    psycho_params = pars[np.argmax(lls)]
    if cnt<2:
        # psychometrics
        # mp_ax.set_ylabel('fraction choose right')
        # mp_ax.set_xlabel('signed contrast')
        # mp_ax.set_title('motor cortex inactivation')
        mp_ax.set_ylim(-.05,1.05)
        mp_ax.plot(xx,erf_psycho_2gammas(psycho_params,xx),label=area_labels[cnt],color=area_colors[cnt])
        mp_ax.errorbar(stims,prop_CCW,yerr=CCW_std,elinewidth=1,lw=0,marker='.',color=area_colors[cnt])
        # chronometrics
        # mc_ax.set_ylabel('median RT')
        # mc_ax.set_xlabel('signed contrast')
        # mc_ax.set_title('motor cortex inactivation')
        mc_ax.set_ylim(.20,.37)
        mc_ax.errorbar(stims,rts,yerr=rt_std,elinewidth=1,marker='.',color=area_colors[cnt],label=area_labels[cnt])
    else:
        # vp_ax.set_ylabel('fraction choose right')
        # vp_ax.set_xlabel('signed contrast')
        # vp_ax.set_title('visual cortex inactivation')
        vp_ax.set_ylim(-.05,1.05)
        vp_ax.plot(xx,erf_psycho_2gammas(psycho_params,xx),label=area_labels[cnt],color=area_colors[cnt])
        vp_ax.errorbar(stims,prop_CCW,yerr=CCW_std,elinewidth=1,lw=0,marker='.',color=area_colors[cnt])
        # chronometrics
        # vc_ax.set_ylabel('median RT')
        # vc_ax.set_xlabel('signed contrast')
        # vc_ax.set_title('visual cortex inactivation')
        vc_ax.set_ylim(.20,.37)
        vc_ax.errorbar(stims,rts,yerr=rt_std,elinewidth=1,marker='.',color=area_colors[cnt],label=area_labels[cnt])

# get control data and psychometric
control_data=bigData[bigData['laserPosY']<-6]
choice_RT[-1,0] = np.median(control_data[control_data['trial_response_choice']=='CCW']['trial_response_time'] - \
                  control_data[control_data['trial_response_choice']=='CCW']['trial_go_cue_trigger_time'])
choice_RT[-1,1] = np.median(control_data[control_data['trial_response_choice']=='CW']['trial_response_time'] - \
                  control_data[control_data['trial_response_choice']=='CW']['trial_go_cue_trigger_time'])
all_choice_RT[-1,0] = control_data[control_data['trial_response_choice']=='CCW']['trial_response_time'] - \
                  control_data[control_data['trial_response_choice']=='CCW']['trial_go_cue_trigger_time']
all_choice_RT[-1,1] = control_data[control_data['trial_response_choice']=='CW']['trial_response_time'] - \
                  control_data[control_data['trial_response_choice']=='CW']['trial_go_cue_trigger_time']
choice_RT_std[-1,0] = np.std(control_data[control_data['trial_response_choice']=='CCW']['trial_response_time'] - \
                control_data[control_data['trial_response_choice']=='CCW']['trial_go_cue_trigger_time'])/np.sqrt(len(control_data[control_data['trial_response_choice']=='CCW']))
choice_RT_std[-1,1] = np.std(control_data[control_data['trial_response_choice']=='CW']['trial_response_time'] - \
                    control_data[control_data['trial_response_choice']=='CW']['trial_go_cue_trigger_time'])/np.sqrt(len(control_data[control_data['trial_response_choice']=='CW']))
stims, cnts = np.unique(control_data['signed_contrast'],return_counts=True)
prop_CCW = []
CCW_std = []
rts=[]
rt_std=[]
for k,stim in enumerate(stims):
    prop_CCW.append(np.mean(control_data[control_data['signed_contrast']==stim]['trial_response_choice']=='CCW'))
    CCW_std.append(np.std(control_data[control_data['signed_contrast']==stim]['trial_response_choice']=='CCW')/np.sqrt(cnts[k]))
    stim_data = control_data[control_data['signed_contrast']==stim]
    rts.append(np.median(stim_data['trial_response_time'] - stim_data['trial_go_cue_trigger_time']))
    rt_std.append(np.std(stim_data['trial_response_time'] - stim_data['trial_go_cue_trigger_time'])/np.sqrt(cnts[k]))

pars,lls = [],[]
for i in range(25):
    parStart = np.array([-5+np.random.rand()*10,0+np.random.rand()*100, 0+np.random.rand(), 0+np.random.rand()])
    params, ll = mle_fit_psycho([stims,cnts,prop_CCW],P_model='erf_psycho_2gammas',
                                parstart=parStart,
                                parmin=np.array([-5, 0., 0., 0.]),
                                parmax=np.array([5, 100., 1, 1]),
                                nfits=10)
    pars.append(params)
    lls.append(ll)
xx = np.linspace(-1,1,100)
psycho_params = pars[np.argmax(lls)]
vp_ax.plot(xx,erf_psycho_2gammas(psycho_params,xx),label='control',color='g')
plt.tight_layout()
mp_ax.plot(xx,erf_psycho_2gammas(psycho_params,xx),label='control',color='g')
plt.tight_layout()
vp_ax.errorbar(stims,prop_CCW,yerr=CCW_std,lw=0,marker='.',elinewidth=1,color='g')
plt.tight_layout()
mp_ax.errorbar(stims,prop_CCW,yerr=CCW_std,lw=0,marker='.',elinewidth=1,color='g')
plt.tight_layout()
vc_ax.errorbar(stims,rts,yerr=rt_std,elinewidth=1,marker='.',color='g',label='control')
plt.tight_layout()
mc_ax.errorbar(stims,rts,yerr=rt_std,elinewidth=1,marker='.',color='g',label='control')
plt.tight_layout()
# for ax in [vp_ax,mp_ax,vc_ax,mc_ax]:
#     ax.legend()
plt.tight_layout()

# RT by choice
fig,ax = plt.subplots(1,1,figsize=(2.5,2.5))
w=.8/3
ax.bar(np.arange(2)-w,choice_RT[0],width=w, color='b',label='laser left')
ax.bar(np.arange(2),choice_RT[1],width=w, color='r' ,label='laser right')
ax.bar(np.arange(2)+w,choice_RT[-1],width=w, color='g' ,label='control')

ax.set_xticks(np.arange(2))
ax.set_xticklabels(['left choice','right choice'])
# ax.set_title('motor cortex inhibition')
ax.set_ylim(.15,.35)
ax.errorbar(np.arange(2)-w,choice_RT[0],yerr=choice_RT_std[0], elinewidth=1,lw=0,color='k')
ax.errorbar(np.arange(2),choice_RT[1],yerr=choice_RT_std[1], elinewidth=1,lw=0,color='k')
ax.errorbar(np.arange(2)+w,choice_RT[-1],yerr=choice_RT_std[-1], elinewidth=1,lw=0,color='k')
# ax.legend()
plt.tight_layout()

fig,ax = plt.subplots(1,1,figsize=(2.5,2.5))
w=.8/3
ax.bar(np.arange(2)-w,choice_RT[2],width=w, color='b',label='laser left')
ax.bar(np.arange(2),choice_RT[3],width=w, color='r' ,label='laser right')
ax.bar(np.arange(2)+w,choice_RT[-1],width=w, color='g' ,label='control')

ax.set_xticks(np.arange(2))
ax.set_xticklabels(['left choice','right choice'])
# ax.set_title('visual cortex inhibition')
ax.set_ylim(.15,.35)
ax.errorbar(np.arange(2)-w,choice_RT[2],yerr=choice_RT_std[2], elinewidth=1,lw=0,color='k')
ax.errorbar(np.arange(2),choice_RT[3],yerr=choice_RT_std[3], elinewidth=1,lw=0,color='k')
ax.errorbar(np.arange(2)+w,choice_RT[-1],yerr=choice_RT_std[-1], elinewidth=1,lw=0,color='k')
# ax.legend()
plt.tight_layout()

## Stats
test_stats=np.zeros((4,2,2))
for i in range(4):
    u,p= stats.ranksums(all_choice_RT[i,0],all_choice_RT[-1,0])
    test_stats[i,0,0] = u
    test_stats[i,0,1] = p*8
    u,p = stats.ranksums(all_choice_RT[i,1],all_choice_RT[-1,1])
    test_stats[i,1,0] = u
    test_stats[i,1,1] = p*8
print('left choice:\n', test_stats[:,0])

print('right choice:\n', test_stats[:,1])

plt.show()